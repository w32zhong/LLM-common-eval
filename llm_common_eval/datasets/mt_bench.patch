diff --git a/fastchat/llm_judge/common.py b/fastchat/llm_judge/common.py
index d2640d6..321a8a6 100644
--- a/fastchat/llm_judge/common.py
+++ b/fastchat/llm_judge/common.py
@@ -164,14 +164,34 @@ def run_judge_single(question, answer, judge, ref_answer, multi_turn=False):
     conv.append_message(conv.roles[1], None)
 
     if model in OPENAI_MODEL_LIST:
-        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)
-    elif model in ANTHROPIC_MODEL_LIST:
-        judgment = chat_completion_anthropic(
-            model, conv, temperature=0, max_tokens=1024
-        )
+    #    judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)
+    #elif model in ANTHROPIC_MODEL_LIST:
+    #    judgment = chat_completion_anthropic(
+    #        model, conv, temperature=0, max_tokens=1024
+    #    )
+    #elif model == 'gemini':
+        import time
+        import google.generativeai as genai
+        GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
+        assert GOOGLE_API_KEY is not None
+        genai.configure(api_key=GOOGLE_API_KEY)
+        model = genai.GenerativeModel('gemini-1.0-pro')
+        prompt = conv.get_prompt()
+        while True:
+            try:
+                print('[question]', question["question_id"])
+                response = model.generate_content(prompt)
+                print(response.text)
+                judgment = response.text
+                break
+            except Exception as e:
+                print('[retry]', e)
+        time.sleep(0.5)
     else:
         raise ValueError(f"Invalid judge model name: {model}")
 
+    #breakpoint()
+
     if judge.prompt_template["output_format"] == "[[rating]]":
         match = re.search(one_score_pattern, judgment)
         if not match:
diff --git a/fastchat/llm_judge/gen_judgment.py b/fastchat/llm_judge/gen_judgment.py
index a1c70b2..4626f0d 100644
--- a/fastchat/llm_judge/gen_judgment.py
+++ b/fastchat/llm_judge/gen_judgment.py
@@ -9,6 +9,8 @@ import json
 import numpy as np
 from tqdm import tqdm
 
+import sys
+sys.path.insert(0, '../..')
 from fastchat.llm_judge.common import (
     load_questions,
     load_model_answers,
@@ -121,6 +123,11 @@ def make_match_single(
         for i in range(len(models)):
             q_id = q["question_id"]
             m = models[i]
+
+            # MODIFIED: to enable partial eval
+            if q_id not in model_answers[m]:
+                continue
+
             a = model_answers[m][q_id]
             if ref_answers is not None:
                 ref = ref_answers[judge.model_name][q_id]
@@ -252,7 +259,8 @@ if __name__ == "__main__":
             make_match_func = make_match
             baseline_model = args.baseline_model
 
-    check_data(questions, model_answers, ref_answers, models, judges)
+    # MODIFIED: skip checking data completeness (to enable partial eval)
+    ### check_data(questions, model_answers, ref_answers, models, judges)
 
     question_math = [q for q in questions if q["category"] in NEED_REF_CATS]
     question_default = [q for q in questions if q["category"] not in NEED_REF_CATS]
@@ -301,7 +309,7 @@ if __name__ == "__main__":
     # Show match stats and prompt enter to continue
     print("Stats:")
     print(json.dumps(match_stat, indent=4))
-    input("Press Enter to confirm...")
+    #input("Press Enter to confirm...")
 
     # Play matches
     if args.parallel == 1:
diff --git a/fastchat/llm_judge/show_result.py b/fastchat/llm_judge/show_result.py
index f20801b..20fe132 100644
--- a/fastchat/llm_judge/show_result.py
+++ b/fastchat/llm_judge/show_result.py
@@ -4,6 +4,7 @@ python3 show_result.py --mode [single|pairwise-baseline|pairwise-all]
 """
 import argparse
 import pandas as pd
+pd.set_option('display.max_colwidth', None)
 
 
 def display_result_single(args):
